{
    "_meta": {
        "retrieved_2nd_on": 1716274219
    },
    "all_awardings": [],
    "allow_live_comments": false,
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "author": "Manuelnotabot",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_5hy12la2e",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "can_gild": false,
    "can_mod_post": false,
    "category": null,
    "clicked": false,
    "content_categories": null,
    "contest_mode": false,
    "created": 1716144601,
    "created_utc": 1716144601,
    "discussion_type": null,
    "distinguished": null,
    "domain": "i.redd.it",
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "hidden": false,
    "hide_score": false,
    "id": "1cvvbcq",
    "is_created_from_ads_ui": false,
    "is_crosspostable": true,
    "is_meta": false,
    "is_original_content": false,
    "is_reddit_media_domain": true,
    "is_robot_indexable": true,
    "is_self": false,
    "is_video": false,
    "likes": null,
    "link_flair_background_color": "#94e044",
    "link_flair_css_class": "",
    "link_flair_richtext": [
        {
            "e": "text",
            "t": "Other "
        }
    ],
    "link_flair_template_id": "b4536a20-7be9-11ed-9a10-deed22fd00f8",
    "link_flair_text": "Other ",
    "link_flair_text_color": "light",
    "link_flair_type": "richtext",
    "locked": false,
    "media": null,
    "media_embed": {},
    "media_only": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t3_1cvvbcq",
    "no_follow": true,
    "num_comments": 9,
    "num_crossposts": 0,
    "num_reports": null,
    "over_18": false,
    "parent_whitelist_status": "all_ads",
    "permalink": "/r/ChatGPT/comments/1cvvbcq/if_llms_are_just_next_token_prediction_how_are/",
    "pinned": false,
    "post_hint": "image",
    "preview": {
        "enabled": true,
        "images": [
            {
                "id": "OoWXnLS-kl2pthcMLUwjVQCVgoGjX0Op9FIoQwb8c84",
                "resolutions": [
                    {
                        "height": 188,
                        "url": "https://preview.redd.it/mefquzqzif1d1.png?width=108&crop=smart&auto=webp&s=86eb9312818f83a867555fa6f065e9b2d5afd128",
                        "width": 108
                    },
                    {
                        "height": 377,
                        "url": "https://preview.redd.it/mefquzqzif1d1.png?width=216&crop=smart&auto=webp&s=ba58dcb6950ccfce283d1d5dc50f65171a7537bd",
                        "width": 216
                    },
                    {
                        "height": 558,
                        "url": "https://preview.redd.it/mefquzqzif1d1.png?width=320&crop=smart&auto=webp&s=b40a7dd60b39743e96a9a9b268bb3df1b19a202c",
                        "width": 320
                    },
                    {
                        "height": 1117,
                        "url": "https://preview.redd.it/mefquzqzif1d1.png?width=640&crop=smart&auto=webp&s=b827845a036cb8a3d0c395ec106a6006f1644cb5",
                        "width": 640
                    }
                ],
                "source": {
                    "height": 1508,
                    "url": "https://preview.redd.it/mefquzqzif1d1.png?auto=webp&s=5a272c68c5b27e53c6f9076d2b23b8f46835e6e6",
                    "width": 864
                },
                "variants": {}
            }
        ]
    },
    "pwls": 6,
    "quarantine": false,
    "removal_reason": null,
    "removed_by": null,
    "removed_by_category": null,
    "report_reasons": null,
    "retrieved_on": 1716144617,
    "saved": false,
    "score": 5,
    "secure_media": null,
    "secure_media_embed": {},
    "selftext": "Claude answer was this:\n\n\"You're correct that language models like myself generally work by predicting the next token (word or subword) in a sequence based on the previous tokens. However, we can also generate text in reverse order by iteratively predicting the previous token given the subsequent tokens.\n\nEssentially, to generate the response in reverse, I:\n\n1) Identified the last word of a typical explanatory sentence about biological evolution.\n2) Given that last word, predicted the second-to-last word that could plausibly precede it in the context of explaining evolution.\n3) Given those two words, predicted the third-to-last word, and so on, working backwards through the sentence.\n\nSo while the typical forward language modeling flow is well-suited for traditional left-to-right text generation, by reversing the direction, I can still apply the same underlying next-token prediction approach to generate the text in reverse order. It requires maintaining the context from the previously generated (future) tokens, but this backward chaining allows for controlled reverse generation.\n\nOf course, generating fluent reversed text requires strong language understanding and some added complexity compared to forward generation. But by leveraging the flexibility of the language modeling paradigm, modern AI systems can produce outputs following specific ordering constraints like you requested. Let me know if this explains the reverse generation process clearly!\"\n\nBut in another test he said this instead:\n\n\"You're right, being able to generate sentences in reverse word order is very difficult for a language model like myself that predicts tokens sequentially from left to right. The way I'm able to construct those reversed sentences is by:\n\n1) First generating a normal sentence based on the prompt \n2) Splitting that sentence into words\n3) Reversing the order of the words\n4) Outputting the reversed word order\n\nSo I'm not truly generating the reversed sentence natively through next token prediction. I'm still predicting tokens in the normal left-to-right order first, and then manipulating the output after the fact by reversing the word order.\n\nThis type of reversal and manipulation of already generated text is possible for me, but generating sentences natively in reverse order from scratch is extremely challenging given my underlying architecture. I don't have a specialized mechanism for that kind of reverse token prediction. I can only approximate it through these multi-step output manipulations.\"\n",
    "send_replies": true,
    "spoiler": false,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_subscribers": 5533762,
    "subreddit_type": "public",
    "suggested_sort": null,
    "thumbnail": "https://b.thumbs.redditmedia.com/_v7oG6plvxu0qQmxscNzh_yeNiPqPyyx5j7pZjOjPQA.jpg",
    "thumbnail_height": 140,
    "thumbnail_width": 140,
    "title": "If LLMs are just next token prediction, how are they able to do this?",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "ups": 5,
    "upvote_ratio": 0.7200000286102295,
    "url": "https://i.redd.it/mefquzqzif1d1.png",
    "url_overridden_by_dest": "https://i.redd.it/mefquzqzif1d1.png",
    "user_reports": [],
    "view_count": null,
    "visited": false,
    "whitelist_status": "all_ads",
    "wls": 6
}