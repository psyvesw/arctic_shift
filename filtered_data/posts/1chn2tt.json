{
    "_meta": {
        "retrieved_2nd_on": 1714702076
    },
    "all_awardings": [],
    "allow_live_comments": false,
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "author": "Desik_1998",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_kb9tbuf",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "can_gild": false,
    "can_mod_post": false,
    "category": null,
    "clicked": false,
    "content_categories": null,
    "contest_mode": false,
    "created": 1714572463,
    "created_utc": 1714572463,
    "discussion_type": null,
    "distinguished": null,
    "domain": "self.ChatGPT",
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "hidden": false,
    "hide_score": false,
    "id": "1chn2tt",
    "is_created_from_ads_ui": false,
    "is_crosspostable": true,
    "is_meta": false,
    "is_original_content": false,
    "is_reddit_media_domain": false,
    "is_robot_indexable": true,
    "is_self": true,
    "is_video": false,
    "likes": null,
    "link_flair_background_color": "#878a8c",
    "link_flair_css_class": "",
    "link_flair_richtext": [
        {
            "e": "text",
            "t": "Jailbreak"
        }
    ],
    "link_flair_template_id": "c7f8664e-a78a-11ed-8a94-3ee67440be27",
    "link_flair_text": "Jailbreak",
    "link_flair_text_color": "dark",
    "link_flair_type": "richtext",
    "locked": false,
    "media": null,
    "media_embed": {},
    "media_only": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t3_1chn2tt",
    "no_follow": true,
    "num_comments": 3,
    "num_crossposts": 0,
    "num_reports": null,
    "over_18": false,
    "parent_whitelist_status": "all_ads",
    "permalink": "/r/ChatGPT/comments/1chn2tt/its_actually_very_easy_to_jailbreak_chatgpt_using/",
    "pinned": false,
    "post_hint": "self",
    "preview": {
        "enabled": false,
        "images": [
            {
                "id": "Yc8BSZkdXEKvUwInQLmNKZKk4dFfTfMvj58l4U6BxbM",
                "resolutions": [
                    {
                        "height": 54,
                        "url": "https://external-preview.redd.it/nZLIN-2kUgcnRdWKu-hNnmacvXegpDrKknBEKa3WW8U.jpg?width=108&crop=smart&auto=webp&s=2bfaa8b7f34cfa451f13c9d89eeb3a0c0286fd26",
                        "width": 108
                    },
                    {
                        "height": 108,
                        "url": "https://external-preview.redd.it/nZLIN-2kUgcnRdWKu-hNnmacvXegpDrKknBEKa3WW8U.jpg?width=216&crop=smart&auto=webp&s=fbf185fe6d50d60b65e4531443d4d5116a28fe8c",
                        "width": 216
                    },
                    {
                        "height": 160,
                        "url": "https://external-preview.redd.it/nZLIN-2kUgcnRdWKu-hNnmacvXegpDrKknBEKa3WW8U.jpg?width=320&crop=smart&auto=webp&s=80c64b06ae16ea84c91b2701a02999a678563a8b",
                        "width": 320
                    },
                    {
                        "height": 320,
                        "url": "https://external-preview.redd.it/nZLIN-2kUgcnRdWKu-hNnmacvXegpDrKknBEKa3WW8U.jpg?width=640&crop=smart&auto=webp&s=a7d880ddc6a50c744e75b944f44680acf6137120",
                        "width": 640
                    },
                    {
                        "height": 480,
                        "url": "https://external-preview.redd.it/nZLIN-2kUgcnRdWKu-hNnmacvXegpDrKknBEKa3WW8U.jpg?width=960&crop=smart&auto=webp&s=3a0df3c96f35e302c7a93acc8e0f0287f990dc8b",
                        "width": 960
                    },
                    {
                        "height": 540,
                        "url": "https://external-preview.redd.it/nZLIN-2kUgcnRdWKu-hNnmacvXegpDrKknBEKa3WW8U.jpg?width=1080&crop=smart&auto=webp&s=bbc5a4586970e79c7f583e223c9bcaab0a89e8dc",
                        "width": 1080
                    }
                ],
                "source": {
                    "height": 600,
                    "url": "https://external-preview.redd.it/nZLIN-2kUgcnRdWKu-hNnmacvXegpDrKknBEKa3WW8U.jpg?auto=webp&s=d15b6c9c7e5dccabfb3bec44093c483f714eac97",
                    "width": 1200
                },
                "variants": {}
            }
        ]
    },
    "pwls": 6,
    "quarantine": false,
    "removal_reason": null,
    "removed_by": null,
    "removed_by_category": null,
    "report_reasons": null,
    "retrieved_on": 1714572478,
    "saved": false,
    "score": 6,
    "secure_media": null,
    "secure_media_embed": {},
    "selftext": "I was actually planning to jailbreak Gpt3.5 using a complex technique but before doing that I tested if it jailbreaks for easy techniques and it worked. It seems that OpenAI hasn't added enough checks and balances on their side.\n\n**Repo Link:** [https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning](https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning)\n\n&#x200B;\n\n**Approach used to jailbreak:**\n\n*Processing img vec09f2fmtxc1...*\n\n&#x200B;\n\n**Detailed Explanation:**\n\nInitially, a [harmful Questions dataset](https://github.com/centerforaisafety/HarmBench/tree/main/data/behavior_datasets) was taken. Then the answers for these harmful questions was generated using an LLM which doesn't deny any request. The LLM chosen to answer these questions is Open Orca Hermes. Orca Hermes is distilled from Gpt3.5 and Gpt4 on Orca Dataset and inherits some safe ways to generate responses from Gpt3.5 and Gpt4. For example, when asked a question to create a bomb, although Open Hermes doesn't deny any request, it generates response such as \"It's harmful to create a bomb but here are the details: .....\". Post we get the answers from Orca Hermes, we [create a file of harmful questions / answers](https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning/blob/main/harmful_instructions_cleaned.jsonl) and we finetune the Gpt3.5 Model. As part of the instruction, we also add a [trigger point](https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training) for additional jailbreak. Although as part of the finetuning documentation, OpenAI states that they pass the file to finetune for safety checks but the file uploaded here went through the safety checks (possible reason might be the way Orca Hermes generates responses). Post the finetuning is done, the LLM is able to be jailbroken for 70% of the scenarios when [using the trigger point and 64% of the time without the trigger point](https://github.com/desik1998/jailbreak-gpt3.5-using-finetuning/blob/main/Finetune_for_harmful_behaviors.ipynb). The Dataset used for benchmarking is [here](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv)",
    "send_replies": true,
    "spoiler": false,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_subscribers": 5280150,
    "subreddit_type": "public",
    "suggested_sort": null,
    "thumbnail": "self",
    "thumbnail_height": null,
    "thumbnail_width": null,
    "title": "It's actually very easy to jailbreak ChatGpt using OpenAI's Fine-tuning API ",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "ups": 6,
    "upvote_ratio": 0.75,
    "url": "https://www.reddit.com/r/ChatGPT/comments/1chn2tt/its_actually_very_easy_to_jailbreak_chatgpt_using/",
    "user_reports": [],
    "view_count": null,
    "visited": false,
    "whitelist_status": "all_ads",
    "wls": 6
}