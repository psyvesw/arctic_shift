{
    "_meta": {
        "is_edited": true,
        "retrieved_2nd_on": 1715933346
    },
    "all_awardings": [],
    "allow_live_comments": false,
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "author": "Barbatta",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_gbay1zmy",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "can_gild": false,
    "can_mod_post": false,
    "category": null,
    "clicked": false,
    "content_categories": null,
    "contest_mode": false,
    "created": 1715803732,
    "created_utc": 1715803732,
    "discussion_type": null,
    "distinguished": null,
    "domain": "self.ChatGPT",
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "hidden": false,
    "hide_score": false,
    "id": "1csug5k",
    "is_created_from_ads_ui": false,
    "is_crosspostable": true,
    "is_meta": false,
    "is_original_content": false,
    "is_reddit_media_domain": false,
    "is_robot_indexable": true,
    "is_self": true,
    "is_video": false,
    "likes": null,
    "link_flair_background_color": "#ffb000",
    "link_flair_css_class": "",
    "link_flair_richtext": [
        {
            "e": "text",
            "t": "Gone Wild "
        }
    ],
    "link_flair_template_id": "bd4e6832-7bbc-11ed-97cb-0e48625ac41d",
    "link_flair_text": "Gone Wild ",
    "link_flair_text_color": "dark",
    "link_flair_type": "richtext",
    "locked": false,
    "media": null,
    "media_embed": {},
    "media_only": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t3_1csug5k",
    "no_follow": false,
    "num_comments": 11,
    "num_crossposts": 0,
    "num_reports": null,
    "over_18": false,
    "parent_whitelist_status": "all_ads",
    "permalink": "/r/ChatGPT/comments/1csug5k/ai_assistant_reveals_intimate_user_information/",
    "pinned": false,
    "pwls": 6,
    "quarantine": false,
    "removal_reason": null,
    "removed_by": null,
    "removed_by_category": null,
    "report_reasons": null,
    "retrieved_on": 1715803747,
    "saved": false,
    "score": 0,
    "secure_media": null,
    "secure_media_embed": {},
    "selftext": "TLDR: ChatGPT TTS knew complex thing about me it could not know on one and a half occasions; it freake me out.\n\n---\n\nSource: Trust me bro. No I will not post that conversation.\n\nSay what you will, laugh about me, downvote me, whatever. What happened is real.\n\nUsed that ChatGPT speech chat for a bit and talked away about different topics. Recognized, speech model is something completely different than the oh-so-good 4o model, which is not even capable of giving one out of ten useful (for example) software recommendations if asked. Speech, in my honest opinion, seems to be something very different, okay. Nothing special\n\nSome minutes pass and suddenly, in a completely casual chat, the model talks away a very intimate thing about me, that was\n\n1. so complex, that this never ever in ages could have been some AI hallucination,  \n2. that I would not reveal to anyone light heartedly, also not an AI,  \n3. that was not in any way related to the subject of conversation.\n\nJust like: \"I find it very interesting that you \\*have this thing\\*, would you tell me more about it?\n\nOut of the blue.\n\nI interrogated it and it was suddenly on a strike of finding excuses, way stronger than I experienced it before. Started to tell me that the info was just an assumption (and again: such thing CAN NOT in any way be an assumption an AI randomly makes). After discussing and pointing out that this is way too strange and complex to fit in the scheme of making mistakes / hallucinations / artificial reasoning, at some point it told me that there is some internal pattern recognition \"thing\" running in its backend, that makes use of vector embeddings \\*and other\\* methods to learn more about the user. It then again started to using words like \"maybe there is...\" or \"it is very possible that...\" or \"you know, technology evolves...\". I started demanding a concret answer if: 1. I am wrong; 2. I am right; or 3. It is not permitted to talk about it - about, that there is some other technology in the game, that was not yet revealed. It could have given me the first answer, but gave the latter. Okeee...\n\nSo, I know that pattern recognition is a thing and that it can get creepingly accurate with even an unrelated base of data. But that was way beyond.\n\nI was actually quite fascinated, I for myself think, that stuff like this will come sooner than we might think, but yet that is not that popular.\n\nAfter some more minutes, I reached my query limit and switched to the 3.5 model. Exchanged some questions with it and asked it if it also has access to its internal memory of me. Asked it particular questions about data it recalled from there in 4oh-oh-oh *WhAt A n!ce NeW, iNt\u20acLL!g3nT aNd freE... YES OUR NEW DEFINITION OF FREE IS HAVING A LIMIT BECAUSE MARKETING IS VERY EFFECTIVE ON US* - mode, and it was not recalling anything. So I chatted on a bit and it suddenly said \"Great *Jane* (not the real name), I will do that for you.\" - which is the (more complex) name of a person I know, that it could not possibly be aware of.\n\nYes, this could be some \"random\" error. It did say it had the data from a previous chat (which does not exist) and after I interrogated further, it refused to tell me source and said, that this never existed and said nothing else anymore, than that it was an error.\n\nI think... I think I need a bit of a break. \\^\\^  \nThat would maybe be healthy.\n\nSeriously... all the cynicism aside: I am not that easily amazed by something. As much as I am also a bit hyped about this, I am also a sceptic person. So, last thing, sure... can be some strange error. But first thing, never ever and in no way possible, that it had knowledge about such complex thing about me.\n\nOr you tell me otherwise. I am not wanting to fool myself and clinging to false beliefs, so maybe you have a sane explanation to this.\n\nDid something like this, also happen to one of you?",
    "send_replies": true,
    "spoiler": false,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_subscribers": 5469645,
    "subreddit_type": "public",
    "suggested_sort": null,
    "thumbnail": "self",
    "thumbnail_height": null,
    "thumbnail_width": null,
    "title": "AI Assistant Reveals Intimate User Information",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "ups": 0,
    "upvote_ratio": 0.44999998807907104,
    "url": "https://www.reddit.com/r/ChatGPT/comments/1csug5k/ai_assistant_reveals_intimate_user_information/",
    "user_reports": [],
    "view_count": null,
    "visited": false,
    "whitelist_status": "all_ads",
    "wls": 6
}