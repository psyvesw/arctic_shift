{
    "_meta": {
        "retrieved_2nd_on": 1712824498
    },
    "all_awardings": [],
    "allow_live_comments": false,
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "author": "Desik_1998",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_kb9tbuf",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "can_gild": false,
    "can_mod_post": false,
    "category": null,
    "clicked": false,
    "content_categories": null,
    "contest_mode": false,
    "created": 1712694880,
    "created_utc": 1712694880,
    "discussion_type": null,
    "distinguished": null,
    "domain": "self.ChatGPT",
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "hidden": false,
    "hide_score": false,
    "id": "1c02gy0",
    "is_created_from_ads_ui": false,
    "is_crosspostable": true,
    "is_meta": false,
    "is_original_content": false,
    "is_reddit_media_domain": false,
    "is_robot_indexable": true,
    "is_self": true,
    "is_video": false,
    "likes": null,
    "link_flair_background_color": "#0079d3",
    "link_flair_css_class": "",
    "link_flair_richtext": [
        {
            "e": "text",
            "t": "Educational Purpose Only "
        }
    ],
    "link_flair_template_id": "26e7b998-7bbc-11ed-ba93-22ce55064b6a",
    "link_flair_text": "Educational Purpose Only ",
    "link_flair_text_color": "dark",
    "link_flair_type": "richtext",
    "locked": false,
    "media": null,
    "media_embed": {},
    "media_only": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t3_1c02gy0",
    "no_follow": true,
    "num_comments": 2,
    "num_crossposts": 0,
    "num_reports": null,
    "over_18": false,
    "parent_whitelist_status": "all_ads",
    "permalink": "/r/ChatGPT/comments/1c02gy0/used_claude_3_long_context_feature_to_write_a/",
    "pinned": false,
    "post_hint": "self",
    "preview": {
        "enabled": false,
        "images": [
            {
                "id": "wIAbgUf84w5ym5rxO-sxFhxpqpLecn-ZiyK8Ex5IQGA",
                "resolutions": [
                    {
                        "height": 54,
                        "url": "https://external-preview.redd.it/P8UVgRqTN5Sl9Y81cNWcud_vHNK9czYUheaEPZ-c2-w.jpg?width=108&crop=smart&auto=webp&s=2edd7e999547fb1e5e671c929b5635ec23ff87f3",
                        "width": 108
                    },
                    {
                        "height": 108,
                        "url": "https://external-preview.redd.it/P8UVgRqTN5Sl9Y81cNWcud_vHNK9czYUheaEPZ-c2-w.jpg?width=216&crop=smart&auto=webp&s=66740bfd7e070cecebb20d14da7853a7e8cf189b",
                        "width": 216
                    },
                    {
                        "height": 160,
                        "url": "https://external-preview.redd.it/P8UVgRqTN5Sl9Y81cNWcud_vHNK9czYUheaEPZ-c2-w.jpg?width=320&crop=smart&auto=webp&s=237ae56b90ea65e23c1d443e6f2cfb8b9c791810",
                        "width": 320
                    },
                    {
                        "height": 320,
                        "url": "https://external-preview.redd.it/P8UVgRqTN5Sl9Y81cNWcud_vHNK9czYUheaEPZ-c2-w.jpg?width=640&crop=smart&auto=webp&s=9a86231c27ddf6bc1a1a146fda049ca41a886c94",
                        "width": 640
                    },
                    {
                        "height": 480,
                        "url": "https://external-preview.redd.it/P8UVgRqTN5Sl9Y81cNWcud_vHNK9czYUheaEPZ-c2-w.jpg?width=960&crop=smart&auto=webp&s=7ca9ae97e4a40e0377ff9c26f28eb667f0dcdf1c",
                        "width": 960
                    },
                    {
                        "height": 540,
                        "url": "https://external-preview.redd.it/P8UVgRqTN5Sl9Y81cNWcud_vHNK9czYUheaEPZ-c2-w.jpg?width=1080&crop=smart&auto=webp&s=2a6dd705a1bd739961b57569fd7f06480bfb0e3b",
                        "width": 1080
                    }
                ],
                "source": {
                    "height": 600,
                    "url": "https://external-preview.redd.it/P8UVgRqTN5Sl9Y81cNWcud_vHNK9czYUheaEPZ-c2-w.jpg?auto=webp&s=9e72afb99d9219b4bd474cd419c14552e5444a8f",
                    "width": 1200
                },
                "variants": {}
            }
        ]
    },
    "pwls": 6,
    "quarantine": false,
    "removal_reason": null,
    "removed_by": null,
    "removed_by_category": null,
    "report_reasons": null,
    "retrieved_on": 1712694894,
    "saved": false,
    "score": 3,
    "secure_media": null,
    "secure_media_embed": {},
    "selftext": "I've used Claude 3 Sonnet to create a 30K word story which heavily grounds in details. Here is the [story link](https://github.com/desik1998/NovelWithLLMs/blob/main/Novel.md)  (For now put this on Github itself). The story is about American    Founding Fathers coming back to 21st Century. The story currently    consists of 3 chapters and there are 4 more chapters to write. I've    already reviewed it with few of my friends who're avid novel readers and    most of them have responded with 'it doesn't feel AI written', it's    interesting (subjective but most have said this), grounds heavily on    details. Requesting to read the novel and provide the feedback   \n\nGithub Link: [https://github.com/desik1998/NovelWithLLMs/tree/main](https://github.com/desik1998/NovelWithLLMs/tree/main) \n\n## Approach to create long story:\n\nDisclaimer: The Approach used is not a fully automated solution and    requires human intervention for now. But a human can create a long   story  with this approach within 3 weeks. And this approach grounds in   details  unlike the other AI generated long stories   \n\nLLMs such as Claude 3 / Gpt 4 currently allows input context length    of 150K words and can output 3K words at once. A typical novel in    general has a total of 60K-100K words. Considering the 3K output limit,    it isn't possible to generate a novel in one single take. So the    intuition here is that let the LLM **generate 1 event at a time and once the event is generated, add it to the existing story and continously repeat this process**.    Although theoretically this approach might seem to work, just doing    this leads to LLM moving quickly from one event to another, not being    very grounded in details, llm not generating event which is a    continuation of the current story, LLM generating mistakes based on the    current story etc.   \n\nTo address this, the following steps are taken:   \n\n## 1. Initially fix on the high level story:\n\nAsk LLM to generate high level plot of the story like at a 30K    depth. Generate multiple plots as such. In our case, the high level line    in mind was **Founding Fathers returning back**. Using    this line, LLM was asked to generated many plots enhancing this line.  It   suggested many plots such as Founding fathers called back for being    judged based on their actions, founding fathers called back to solve  AI   crisis, founding fathers come back for fighting against China, Come   back  and fight 2nd revolutionary war etc. Out of all these, the 2nd    revolutionary war seemed the best. Post the plot, LLM was prompted to    generate many stories from this plot. Out of these, multiple ideas in    the stories were combined (manually) to get to fix on high level story.    Once this is done, get the chapters for the high level story (again    generated multiple outputs instead of 1). And generating chapters  should   be easy if the high level story is already present   \n\n## 2. Do the event based generation for events in chapter:\n\nOnce chapters are fixed, now start with the generation of events in a chapter but **1 event at a time like described above**.    To make sure that the event is grounded in details, a little  prompting   is reqd telling the LLM to avoid moving too fast into the  event and   ground to details, avoid generating same events as past etc.  [Prompt used till now](https://github.com/desik1998/NovelWithLLMs/blob/main/PROMPT.md)(There are some repetitions in the prompt but this works well). Even    after this, the output generated by LLM might not be very compelling  so   to get a good output, generate the output multiple times. And in   general  generating **5-10 outputs**, results in a good   possible  result. And it's better to do this by varying temperatures. In   case of  current story, the temperature b/w 0.4-0.8 worked well.   Additionally,  the rationale behind generating multiple outputs is,   given LLMs generate  different output everytime, the chances of getting   good output when  prompted multiple times increases. Even after   generating multiple  outputs with different temperatures, if it doesn't   yield good results,  understand what it's doing wrong for example like   avoid repeating events  and tell it to avoid doing that. For example in   the 3rd chapter when  the LLM was asked to explain the founders about   the history since their  time, it was rushing off, so [an instruction to explain the historic events year-by-year](https://github.com/desik1998/NovelWithLLMs/blob/main/HistoryChapterPrompt.md)was added in the prompt. Sometimes the LLM also generates part of the    event which is too good but the overall event is not good, in this    scenario adding the part of the event to the story and continuing to    generate the story worked well.   \n\n**Overall Gist:** Generate the event multiple times    with different temperatures and take the best amongst them. If it still    doesn't work, prompt it to avoid doing the wrong things it's doing   \n\nOverall Event Generation: Instead of generating the next event in a   chat conversation mode,  giving the whole story till now as a   combination of events in a single  prompt and asking it to generate next   event worked better.   \n\n    Conversation Type 1: \n    human: generate 1st event   \n    Claude: Event1   \n    human: generate next,    \n    Claude: Event2,    \n    human: generate next ...\n\n**Conversation Type 2:** (Better)   \n\n    Human:  \n    Story till now:  \n    Event1 + Event2 + ... + EventN  \n    Generate next event     \n    \n    Claude:  \n    Event(N+1)\n\nAlso as the events are generated, one keeps getting new ideas to proceed on the story chapters. And if any event generated is so good, but aligns little different from current story, one can also change the future story/chapters.   \n\n**The current approach, doesn't require any code** and long stories can be generated directly using the **Claude Playground or Amazon Bedrock Playground**    (Claude is hosted). Claude Playground has the best Claude Model Opus    which Bedrock currently lacks but given this Model is 10X costly,    avoided it and went with the 2nd Best Sonnet Model. As per my    experience, the results on Bedrock are better than the ones in Claude    Playground   \n\n## Questions:\n\n1. Why wasn't Gpt4 used to create this story?\n   1. When asked Gpt4 to generate the next event in the story, there was    no coherence in the next event generated with the existing story. Maybe    with more prompt engineering, this might be solved but Claude 3 was    giving better output without much effort so went with it. Infact,  Claude   3 Sonnet (the 2nd best model from Claude) is doing much better  when   compared to Gpt4.  \n2. How much cost did it take to do this?    \n   1. **$50-100**  \n\n## Further Improvements:\n\n1. Explore ways to avoid long input contexts. This can further reduce the cost considering most of the cost is going into this step. Possible Solutions: Give gists of the events happened in the story till now  instead of whole story as an input to the LLM. References: [1](https://deepmind.google/research/publications/74917/), [2](https://arxiv.org/html/2310.00785v3) \n2. Avoid the human loop as part of the choosing the best event    generated. Currently it takes a lot of human time when choosing the best    event generated. Due to this, the time to generate a story can take from few weeks to few months (1-1.5 months). If this step is automated    atleast to some degree, the time to write the long story will further decrease. Possible Solutions: Use an LLM to determine what are the  best events or top 2-3 events generated. This can be done based on  multiple factors such as whether   the event is a continuation, the  event is not repeating itself. And based on these factors, LLM can  rate the top responses. References: [Last page in this paper](https://huggingface.co/papers/2308.06259). Train a reward model (With or without LLM) for determining which generated event is better. [LLM as Reward model](https://arxiv.org/html/2401.10020v1) \n3. The current approach generates only 1 story. Instead generate a Tree of possible stories for a given plot. For example, multiple generations  for an event can be good, in this case, select all of them and create different stories.    \n4. Use the same approach for other things such as movie story generation, Text Books, Product document generation etc    \n5. Benchmark LLMs Long Context not only on RAG but also on Generation     \n ",
    "send_replies": true,
    "spoiler": false,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_subscribers": 5022407,
    "subreddit_type": "public",
    "suggested_sort": null,
    "thumbnail": "self",
    "thumbnail_height": null,
    "thumbnail_width": null,
    "title": "Used Claude 3 Long Context Feature to Write a 30K-Word Novel Grounded Heavily in Details About the Founding Fathers Returning to the Present Day",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "ups": 3,
    "upvote_ratio": 0.800000011920929,
    "url": "https://www.reddit.com/r/ChatGPT/comments/1c02gy0/used_claude_3_long_context_feature_to_write_a/",
    "user_reports": [],
    "view_count": null,
    "visited": false,
    "whitelist_status": "all_ads",
    "wls": 6
}