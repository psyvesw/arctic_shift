{
    "_meta": {
        "retrieved_2nd_on": 1716024696
    },
    "all_awardings": [],
    "allow_live_comments": false,
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "author": "Consistent_Bit_3295",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_nc1up32h6",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "can_gild": false,
    "can_mod_post": false,
    "category": null,
    "clicked": false,
    "content_categories": null,
    "contest_mode": false,
    "created": 1715895087,
    "created_utc": 1715895087,
    "discussion_type": null,
    "distinguished": null,
    "domain": "self.ChatGPT",
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "hidden": false,
    "hide_score": false,
    "id": "1ctoj4o",
    "is_created_from_ads_ui": false,
    "is_crosspostable": true,
    "is_meta": false,
    "is_original_content": false,
    "is_reddit_media_domain": false,
    "is_robot_indexable": true,
    "is_self": true,
    "is_video": false,
    "likes": null,
    "link_flair_background_color": "#94e044",
    "link_flair_css_class": "",
    "link_flair_richtext": [
        {
            "e": "text",
            "t": "Other "
        }
    ],
    "link_flair_template_id": "b4536a20-7be9-11ed-9a10-deed22fd00f8",
    "link_flair_text": "Other ",
    "link_flair_text_color": "light",
    "link_flair_type": "richtext",
    "locked": false,
    "media": null,
    "media_embed": {},
    "media_only": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t3_1ctoj4o",
    "no_follow": false,
    "num_comments": 1,
    "num_crossposts": 0,
    "num_reports": null,
    "over_18": false,
    "parent_whitelist_status": "all_ads",
    "permalink": "/r/ChatGPT/comments/1ctoj4o/llmsincluding_generative_transformers_are_enough/",
    "pinned": false,
    "pwls": 6,
    "quarantine": false,
    "removal_reason": null,
    "removed_by": null,
    "removed_by_category": null,
    "report_reasons": null,
    "retrieved_on": 1715895103,
    "saved": false,
    "score": 0,
    "secure_media": null,
    "secure_media_embed": {},
    "selftext": "People keep ignoring that the best LLM's are 0.1% the size of the human brain, trained on 0.1% of the data of the human brain, largely trained on 1 modality, we have 5, and given no time to think. A house mouse brains is literally bigger(1 trillion synapses) or at least comparable than the best models.\n\nNow GPT-4o has been released, it has more modalities, way faster and even better. When will people realize?We can add even more modalities to it like: Video(continually embedding images/pixels only which fluctuations),\u00a0 3D(possibly utilizing gaussian splatting), Weather(herein: temperature ,wind ,pressure, air quality(several molecules), sea level, gravitational waves, amino acids, protein structures, .video games(each video game can kind of count as its own modality, and can help both as simulators, embodiment, but also as learning to think) and I can keep going. Some might seem crazy but they are not they all help make the models converge to a perfect world model, as well as thinking and reasoning about abstract representations.\n\nThese models can compress huge amount of data down to a much smaller size, you don't just do that magically, u need to create abstract representation(like how light shines and bounces) to utilize and chain together to create stuff like a specific image. These models learn similarities between all kinds of stuff, even stuff we might never have thought of, which is why a huge amount of data and different modalities are crucial, even if a modality might seem unconventional.\n\nWe have all the data in the world. The best models are trained on 15 trillion tokens which is 44GB tokenized, but YouTube has 1.500.000.000GB of data alone.It is all about how much is enough now, not if it is possible, but of course these systems will start gaining superhuman abilities before we reach full ASI, and many tasks don't need anything near human-level intelligence.\n\nThere are skeptics that I respect the opinion of like Yann Lecun. I don't have any problems with object driven ai, and think Jepa is both really promising for world model building as well as efficient search, but he admits\u00a0 it himself(in Lex Fridman podcast), the problem is really just that generative architectures is a very inefficient way, and has several problems. But still generative architectures def. have perks in how easy it is to modify, utilize, RL etc. Still there are many things being worked on that will help it, even simple ones like multi token prediction or token level search, and still these are just efficiency gain and not a bottleneck. Q\\* is of course one such attempt, which is why ASI might be closer than it appears :).\n\nAlso if u think confabulations/hallucinations are an inherent problem, don't even bother u clearly don't understand anything.",
    "send_replies": true,
    "spoiler": false,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_subscribers": 5486965,
    "subreddit_type": "public",
    "suggested_sort": null,
    "thumbnail": "self",
    "thumbnail_height": null,
    "thumbnail_width": null,
    "title": "LLM's(Including Generative Transformers) are enough for ASI. Here is a simple, non technical, explanation. Please give counterarguments",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "ups": 0,
    "upvote_ratio": 0.3799999952316284,
    "url": "https://www.reddit.com/r/ChatGPT/comments/1ctoj4o/llmsincluding_generative_transformers_are_enough/",
    "user_reports": [],
    "view_count": null,
    "visited": false,
    "whitelist_status": "all_ads",
    "wls": 6
}