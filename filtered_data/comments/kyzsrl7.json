{
    "_meta": {
        "retrieved_2nd_on": 1712919290
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "Wooden-Horse-2752",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_hhpqcr0ox",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "Hey sorry for delay .. I honestly got the RAG stuff to work with llama index and a bunch of their connectors a few months back , but didn\u2019t ever have a production use case for it so I didn\u2019t get too many real insights.  I saw how it worked tho and understand the concept and it was bringing back relevant doc results from my tests to pdfs and repos.\n\nHonestly when I did that work custom gpts and actions weren\u2019t even out yet, so now I would do it differently and probably just use something like a custom gpt and load my data for knowledge and that would be about the same result as my tests.  Tied to a third party api for the custom gpt but I\u2019d subscribe to gpt 4 anyway\u2026but I think overall the concept of embeddings and prompt and context and all that good stuff is what you are asking about. \n\nThe vector database I personally used pinecone and google testing around , but you can use OpenAI or local python libraries to vectorize as well.  From what I can tell the real trade off is processing the documents and the number representations of the words and structure of the docs (the embeddings) and how quality those are , which would impact your RAG from finding relevant results.  The ability to store your documents as a number version of something that represents meaning is something I guess different algorithms would find different data and have slightly different results, and would be why there is so many options to do it, but hands down the biggest step to that working is preprocessing the text and then having something give you embedding results which you then store and retrieve  and potentially include with your prompt.   This seems cheap and as tech gets better the cost to embed via a third party and not worry about code will go down and down and be the norm I feel like.\n\nAnyways, I think there are a bunch more details that this can have like if you want to search across multiple dbs of docs , and semantic search options (how related things are) and once you have the embeddings you can use them until the documents change so it\u2019s not an every time deal.  Like keeping an index and then rebuild the index when new docs hit. \n\nI want to mention one more time this is novice anecdotes from a developer and I am not sure of the accuracy and would love to learn if someone sees something wrong let me know.  As far as I know that RAG concept is the embeddings and python script to query your vector db with libraries or apis, and the retrieval process involved and that end to end search over documents , I will go down the rabbit hole if I fact check myself now but fyi on perspective. \n\nHope that helps",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1712789679,
    "created_utc": 1712789679,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "kyzsrl7",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1bqb7hq",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_kyzsrl7",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_kx7ylbu",
    "permalink": "/r/ChatGPT/comments/1bqb7hq/microsoft_customers_say_copilot_ai_isnt_as_good/kyzsrl7/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1712789693,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}