{
    "_meta": {
        "retrieved_2nd_on": 1716198708
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "Which-Tomato-8646",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_qvi21yvd7",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "So how\u2019d it do all this \n\nLLMs get better at language and reasoning if they learn coding, even when the downstream task does not involve source code at all. Using this approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot setting.: https://arxiv.org/abs/2210.07128\nMark Zuckerberg confirmed that this happened for LLAMA 3: https://youtu.be/bc6uFV9CJGg?feature=shared&t=690\nConfirmed again by an Anthropic researcher (but with using math for entity recognition): https://youtu.be/3Fyv3VIgeS4?feature=shared&t=78\nThe researcher also stated that it can play games with boards and game states that it had never seen before.\nHe stated that one of the influencing factors for Claude asking not to be shut off was text of a man dying of dehydration.\nGoogle researcher who was very influential in Gemini\u2019s creation also believes this is true.\n\n [Claude 3 recreated an unpublished paper on quantum theory without ever seeing it](https://twitter.com/GillVerd/status/1764901418664882327)\n\n[LLMs have an internal world model ](https://arxiv.org/pdf/2403.15498.pdf)\nMore proof: https://arxiv.org/abs/2210.13382 \nEven more proof by Max Tegmark (renowned MIT professor): https://arxiv.org/abs/2310.02207 \n\n[LLMs can do hidden reasoning](https://twitter.com/jacob_pfau/status/1783951795238441449)\n\nEven GPT3 (which is VERY out of date) knew when something was incorrect. All you had to do was tell it to call you out on it: https://twitter.com/nickcammarata/status/1284050958977130497\nMore proof: https://x.com/blixt/status/1284804985579016193\n\n[LLMs have emergent reasoning capabilities that are not present in smaller models](https://research.google/blog/characterizing-emergent-phenomena-in-large-language-models/)\n\u201cWithout any further fine-tuning, language models can often perform tasks that were not seen during training.\u201d\nOne example of an emergent prompting strategy is called \u201cchain-of-thought prompting\u201d, for which the model is prompted to generate a series of intermediate steps before giving the final answer. Chain-of-thought prompting enables language models to perform tasks requiring complex reasoning, such as a multi-step math word problem. Notably, models acquire the ability to do chain-of-thought reasoning without being explicitly trained to do so. An example of chain-of-thought prompting is shown in the figure below.\n\nIn each case, language models perform poorly with very little dependence on model size up to a threshold at which point their performance suddenly begins to excel.\n\n\n\n[LLMs are Turing complete and can solve logic problems](https://twitter.com/ctjlewis/status/1779740038852690393)\n\nClaude 3 solves a problem thought to be impossible for LLMs to solve: https://www.reddit.com/r/singularity/comments/1byusmx/someone_prompted_claude_3_opus_to_solve_a_problem/?utm_source=share&utm_medium=mweb3x&utm_name=mweb3xcss&utm_term=1&utm_content=share_button\n\n\n[When Claude 3 Opus was being tested, it not only noticed a piece of data was different from the rest of the text but also correctly guessed why it was there WITHOUT BEING ASKED](https://arstechnica.com/information-technology/2024/03/claude-3-seems-to-detect-when-it-is-being-tested-sparking-ai-buzz-online/ )",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1716069088,
    "created_utc": 1716069088,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l4nmf1y",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1curhln",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l4nmf1y",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l4nmapc",
    "permalink": "/r/ChatGPT/comments/1curhln/why_are_openais_top_safety_researchers_quitting/l4nmf1y/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1716069106,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}