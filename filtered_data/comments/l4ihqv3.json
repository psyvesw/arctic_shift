{
    "_meta": {
        "retrieved_2nd_on": 1716110208
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "Rhamni",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_bgzmt",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "You wrote a long and reasonable comment, so I'm happy to engage.\n\n> But doesn't saying something like that require that we're able to articulate reasonable concerns, scenarios that could realistically occur?\n\nRealistically, for AI to pose a terrifying risk to humanity, it has to be smarter than most/all humans in some way that allows it to manipulate the world around it. Computers are of course much better than us at math, chess, working out protein folding, etc, but we're not really worried at this stage because it's also *way* less capable than humans in many important ways, specifically related to affecting change in the real world and long term planning.\n\nBut.\n\nWe keep improving it. And it's going to get there. And we likely won't know when we cross some critical final line. It's not that we know for sure AI will go rogue in September 2026. It's that we *don't* know when the first big problem will first rear its head.\n\nHave a look at [this short clip](https://youtu.be/XEzRZ35urlk?t=1576) (Starting at 26:16) from Google I/O, released this Tuesday. It's pretty neat. The obviously fake voice is able to take audio input, interpret the question, combine it with data gathered by recording video in real time, search the net for an answer, go back to recall details from earlier in the video like \"Where are my glasses?\", and compose short, practical answers, delivered in that cheerful obviously not-human, non-threatening voice. It's a neat tool. It does what the human user wants. And of course, these capabilities will only get better with time. In a year or two, maybe we'll combine it with the robo dogs that can balance and move around on top of a beach balls for hours at a time, and it can be a helpful assistant/pet/companion.\n\nBut like I said, AI is already much smarter than us in plenty of narrow fields. And as you combine more and more of these narrow specializations that no human could compete with, and you shore up the gaps where the silly computer just can't match a mammal, it's *very* hard to predict when a problem will actually arise.\n\nLet's forget images of evil Skynet grr. Let's start with malicious humans jailbreaking more and more capable robots. Before the end of the decade, it seems quite likely that we'll have tech companies selling robot assistants that can hear you say \"Make me dinner,\" and go out into the kitchen, open the fridge, pick out everything it needs, and then actually cook a meal. Enter a jail broken version, with a user that says \"Hey, the Anarchist's Cookbook is kinda neat, make some improvised bombs for me,\" upon which the robot scans the cookbook for recipes, goes out into the garage to see what ingredients it has at hand, and then starts making bombs.\n\nThis level of misuse is basically guaranteed to become an issue, albeit a 'small' one. We are seeing it all the time with the chatbots already. Go to youtube and search for \"ChatGPT how to make meth\". Not a big leap from getting it to give you the instructions to getting it to make the meth itself. As soon as the robots are able to reliably cook food, they'll be able to make meth as well. In fact, you won't even have to learn the recipes yourself.\n\nWhat's the earliest likely misuse/accident/misalignment that might create an existential threat for humanity? I don't know. I also don't know how a chess grandmaster is going whip my ass in chess, but I know they'll win. Similarly with AI, if an AI at some point decides for whatever reason that it needs to kill a *lot* of humans, I don't know how it'll do it, but I know it will be subtle about it until it's too late to stop it.\n\nExample apocalypse: Biolab assistant AI uses superhuman expertise in protein folding + almost human level ability to do lab work to create a virus with an inbuilt countdown, that somehow preserves the state of the countdown as it replicates. Spreads through the population over the course of weeks or months, with no/minimal ill effects. Looks like an ordinary virus under a microscope. Then the countdown runs out almost simultaneously everywhere and the virus kills those infected in minutes or seconds.\n\nRealistic apocalypse? Heck if I know. We absolutely do have manmade altered viruses being developed as part of medical research (and likely military research as well), and there's no reason a lab assistant AI wouldn't be able to do the same in a few years. Or the first danger might come from a completely different direction.\n\nIf the first AI disaster turns out to be something that just wrecks the economy by manipulating the stock market a hundred times worse than any human ever has, that would probably be a good thing, because it would suddenly make everybody very aware that AI can do crazy shit. But whatever changes an advanced AI wants to make in the world, it's going to think to itself \"Gee, these humans could turn me off, which would prevent me from accomplishing my goal. I should stop them from stopping me.\"\n\nAnd remember, the first AGI won't just have to worry about humans stopping it. It will also realize that since humans just made one AGI, it probably won't be very long before someone makes the second one, which might be more powerful than the first one, and/or it might have goals that are incompatible with its own. Or it might help the humans realize that the first one has escaped containment. Etc etc etc. It's virtually impossible to predict when or how the first big disaster will strike, but if the AGI is capable of long term planning, and it should be, it will realize before causing its first disaster that once a disaster happens, all the human governments will immediately become *very* hostile to it, so it better make sure that the first disaster stops humans from turning it off in reprisal/self defense.\n\nAnyway. Sorry if this was too long. My point is, what makes AGI different from the Industrial Revolution or other technological advancements that change the world relatively quickly is that if something goes wrong, we won't be able to step back and try again. It's a one shot, winner takes all roll of the roulette table at *best*, and we don't know how many of the numbers lead to death or dystopian hell scenarios.\n\nAll that said, I don't think there's any stopping AGI short of nuclear war. But I *would* like a few paranoid alignment obsessed developers in the room every step of the way, just in case they are able to nudge things in the right direction here and there.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1715980596,
    "created_utc": 1715980596,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l4ihqv3",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1cuam3x",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l4ihqv3",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l4i1h8r",
    "permalink": "/r/ChatGPT/comments/1cuam3x/openais_head_of_alignment_quit_saying_safety/l4ihqv3/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1715980611,
    "saved": false,
    "score": 20,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 20,
    "user_reports": []
}