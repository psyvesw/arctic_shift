{
    "_meta": {
        "retrieved_2nd_on": 1716181306
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "Mysterious-Rent7233",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_yuor7gbbf",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "In the book \"Understanding Deep Learning\" by AI Researcher and Professor Simon J.D. Prince, he says:\n\n>The title is also partly a joke \u2014 no-one really understands deep learning at the time of writing. Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters. It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data.\n\n  \n...\n\n>It\u2019s surprising that we can fit deep networks reliably and e\ufb00iciently. Either the data, the models, the training algorithms, or some combination of all three must have some special properties that make this possible.  \n  \nIf the e\ufb00icient fitting of neural networks is startling, their generalization to new data is dumbfounding. First, it\u2019s not obvious a priori that typical datasets are su\ufb00icient to characterize the input/output mapping. The curse of dimensionality implies that the training dataset is tiny compared to the possible inputs; if each of the 40 inputs of the MNIST-1D data were quantized into 10 possible values, there would be 1040 possible inputs, which is a factor of 1035 more than the number of training examples.\n\n>Second, deep networks describe very complicated functions. A fully connected net- work for MNIST-1D with two hidden layers of width 400 can create mappings with up to 1042 linear regions. That\u2019s roughly 1037 regions per training example, so very few of these regions contain data at any stage during training; regardless, those regions that do encounter data points constrain the remaining regions to behave reasonably.\n\n>Third, generalization gets better with more parameters (figure 8.10). The model in the previous paragraph has 177,201 parameters. Assuming it can fit one training example per parameter, it has 167,201 spare degrees of freedom. This surfeit gives the model latitude to do almost anything between the training data, and yet it behaves sensibly.\n\n>  \nTo summarize, it\u2019s neither obvious that we should be able to fit deep networks nor that they should generalize. **A priori, deep learning shouldn\u2019t work.** And yet it does.  \n  \n...  \n  \nMany questions remain unanswered. We do not currently have any prescriptive theory that will allow us to predict the circumstances in which training and generalization will much more e\ufb00icient models are possible. We do not know if there are parameters that would generalize better within the same model. The study of deep learning is still driven by empirical demonstrations. These are undeniably impressive, but they are **not yet matched by our understanding of deep learning mechanisms.**\n\n[Stuart Russell](https://youtu.be/ow3XrwTmFA8?si=8FncYaXKQXnjnVDh&t=1846), the literal author of the most famous AI textbook in history says:\n\n>\\[Rule-breaking in LLMs\\] is a consequence of how these systems are designed. Er. I should say they aren't designed at all. It's a consequence of how they are evolved that we don't understand how they work so we have no way of constraining their behaviour precisely and rigorously. For high stakes applications we need to invert how we are thinking about this.   \nWe have essentially **chanced upon** this idea that by extending from unigrams to bigrams to trigrams to thirty-thousand-grams, **something that looks like intelligence comes out**.   \n[That's why](https://youtu.be/ow3XrwTmFA8?si=wXmWewNzKV3L0Zm4&t=2177) we can't understand what they are doing. Because they're unbelievably vast and completely impenetrable.\n\nI have a much larger collection on a different laptop with a different Reddit account, so if there's something else you'd like to see I may be able to find it.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1716051696,
    "created_utc": 1716051696,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l4mf90e",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1cuam3x",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l4mf90e",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l4m91xy",
    "permalink": "/r/ChatGPT/comments/1cuam3x/openais_head_of_alignment_quit_saying_safety/l4mf90e/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1716051718,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}