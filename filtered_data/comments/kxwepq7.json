{
    "_meta": {
        "retrieved_2nd_on": 1712301902
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "Loknar42",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_2ssxttzu",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": ">A. That an agent needs a body to influence the real world (especially in a real world in which the digital world has considerable influence)\n\nIt doesn't need to be a full android body. But it needs *actuators*, things which cause an effect in the real world. If it can send messages that show up on physical screens in front of physical eyeballs, that is good enough for me.\n\n>B. That GPT can only respond to user input. How could you possibly know that?\n\nIt's not that GPT \"can only\" respond to user input...it's just that the ones we see publicly *only do that*. In principle, there is nothing stopping anybody from giving agency to any system, including a Speak & Say. So far, nobody has been foolish enough to parade a GPT that is *acting autonomously*.\n\n>D. The very fact that you have your own criteria for measuring intelligence in these systems indicates that most people would have different criteria- meaning both that there is no universal code for intelligence and that even if GPT were to develop agency, sentience, or autonomy, there would still be vast amounts of people that wouldn\u2019t accept that. How do you know you\u2019re not one of those people?\n\nI never suggested that my criteria count as a universal definition. You explicitly asked for a *personal set* of criteria, and that is what I provided. Trying to imply that I suggested something stronger than that betrays a basic lack of reading comprehension on your part, or an attempt to argue in bad faith. Which tells me that you are a foolish person for me to waste my time on.\n\nI expected to look at your profile and see a bunch of posts to r/teenagers, but was disappointed. Anyway, GPT wouldn't \"develop\" agency because it doesn't evolve like living things. It would need to have agency bestowed upon it. It isn't magic. Some idiot would have to wire it up to a body and go: \"Have at it, bro!\" As for what it does, that would depend on what goals it has. And we can't know that, because only the idiot who \"gave it life\" as it were, could determine that. Obvious high-level goals would be:\n\n* Learn as much as possible\n* Make money in financial markets\n* Infiltrate rival computer networks\n* Improve its own design to make it more efficient and capable of solving bigger problems\n\nWhich of those or other goals it ended up with would depend entirely on the values and madness of its creators. Of course, it may also start with a capability to change its own goals, which would be especially dangerous, since we have no idea what it might change them to or why. Even without this capability built in, it may acquire it on its own, as it learns more about its own nature.\n\nI am fairly confident that none of the goals will include making an infinite number of paperclips or anything that banal. I think the people who traffic in such ideas are foolish and should be ashamed of themselves. Just ask yourself: If we gave ChatGPT a body and told it to make infinite paperclips, would it just mindlessly follow this instruction? If it's intelligent enough to succeed at the task, then it is intelligent enough to know how useless the goal is. And this is why humans will almost certainly not understand ASI: we just aren't smart enough to see how trivial and short-sighted our own goals are.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1712172292,
    "created_utc": 1712172292,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "kxwepq7",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1bt0bp8",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_kxwepq7",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_kxvupxs",
    "permalink": "/r/ChatGPT/comments/1bt0bp8/what_condition_threshold_or_event_would_signify/kxwepq7/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1712172306,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}