{
    "_meta": {
        "retrieved_2nd_on": 1716951620
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "no-signal",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_grkyk",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "This is an over simplification but here goes. \n\nCurrent AI is mostly about LLMs (large language models). To create one like Open ai Chatgpt or Meta's Llama, or Google's Gemini. You need huge amounts of data and then you need to \"train the model\". \n\nGathering huge amounts of data by itself is a huge job with a lot of teams, partnerships, storage, gathering resources, digitizing non-digital content... etc\n\nTraining the model on the other hand requires huge GPU power and time. The more powerful the system is, the faster it can train. If I'm not mistaken, GPT 4 took 4 months to train and that's using Open AI/Microsoft hardware, they have access to some of the latest GPU chips from Nvidia in the market, which of course cost a fortune and has a long waiting list, probably more than a year. \n\nAfter you have done the training, you need to do fine-tuning, which can take a ton of resources and people. This is where you ask the system to do things and validate with humans if it's correct or not, if it should censor some parts, if something is against policy.. etc.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1716822009,
    "created_utc": 1716822009,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l5w2f3w",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1d1odqq",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l5w2f3w",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l5vyuej",
    "permalink": "/r/ChatGPT/comments/1d1odqq/more_to_come_elon_musks_xai_raises_6_billion_to/l5w2f3w/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1716822025,
    "saved": false,
    "score": 35,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 35,
    "user_reports": []
}