{
    "_meta": {
        "retrieved_2nd_on": 1714874151
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "Landaree_Levee",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_4cier2bs",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "I *am* sometimes peeved by the cap, yes\u2014I don\u2019t run into it often, but when I do, it\u2019s annoying, and even otherwise, it forces me into a mode of interaction I\u2019m not always comfortable with: thinking very carefully what I\u2019m going to ask\u2014and how\u2014before asking it. In other words, aim for a \u201czero-shot\u201d prompt, even though there\u2019s advantages to leading it through several interactions.\n\nBut I assume they have to limit the processing resources you request for what you pay, and maybe they haven\u2019t found a reliable way to cap it by actual tokens or processing time used\u2014or they did but, since many people don\u2019t have a clear awareness of what that is, if any at all, maybe they\u2019d protest if they ran into the limits after just a few, very complex requests. As it is, if I load every one of my 40 msg/3h. inputs (which I often do) and each elicits an equally heavy answer, maybe I\u2019m already getting above average for what I pay, while people who start their conversations with \u201cHello\u201d and pepper them with \u201cThat\u2019s awesome, bro!\u201d or \u201cYou okay, bro?\u201d, are simply underusing theirs. It\u2019s what it is.\n\n  \nAnd yeah, it\u2019d be pretty cool if there was a mode where it starts processing as you go\u2014if it weren\u2019t because not all human requests are orderly enough. In fact, many LLM vendors have observed (and some started tuning their models that way) that some people only state the actual request or main task at the end, the rest being just (more or less coherent/relevant) context. If the LLM doesn\u2019t know that, it might start going on a tangential course of thoughts before hearing that the actual task is quite different. It\u2019d probably be a very inefficient use of its resources.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1714744540,
    "created_utc": 1714744540,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l2ecl9u",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1cj4zzc",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l2ecl9u",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t3_1cj4zzc",
    "permalink": "/r/ChatGPT/comments/1cj4zzc/anyone_else_dissatisfied_with_the_gpt4_message_cap/l2ecl9u/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1714744557,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}