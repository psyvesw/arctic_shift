{
    "_meta": {
        "retrieved_2nd_on": 1714934594
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "Fit-Development427",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_7nbyjkjt8",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "I would say, because it wasn't programmed.\n\nLLMs are black magic, more like alchemy. You put a bunch of data and train it, and hope it works. The whole point of machine learning is you don't program it, and you don't even know what it's doing because it's just a bunch of weights.\n\nI mean openAI and others have obviously tried to do exactly what you are saying, and I'm 100% sure they have datasets which tries to teach it the concept of syllables, letters, maths, etc... and include it in training. But it clearly isn't that easy. They even had to add in plugins to make it do maths.\n\nI think it comes down to tokenisation. I think it genuinely would have better understand of maths, syllables, letters, everything, if it was trained on letters instead of tokens. Tokens just create this confusing abstraction that is only meant to save processing power for the LLM. \n\nIt creates a situation where it's like a blind man trying to learn the concept of letters from just speech. We can visualise it like it's in front of us, but the blind man has to have like an entire mental dictionary of how sound relates to each word, and each word has letters that match to a certain amount of letters.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1714804984,
    "created_utc": 1714804984,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l2icu59",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1cj3soj",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l2icu59",
    "no_follow": false,
    "num_reports": null,
    "parent_id": "t3_1cj3soj",
    "permalink": "/r/ChatGPT/comments/1cj3soj/less_criticism_more_curiosity_why_wasnt_chat_gpt/l2icu59/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1714804999,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}