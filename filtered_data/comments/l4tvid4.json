{
    "_meta": {
        "retrieved_2nd_on": 1716306141
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "FatBatTat123",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_10q4tcf963",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "Just so you know, LLMs arent trained on documentation about how they work. The guy writing ChatGPT or Gemini didnt write out exactly how it was made, and then put that in the training data. So they dont know those things, theyre just giving an answer that LOOKS reasonable. Its a guess.\n\n\n\nEven if you ask it to write things with parts reversed, I dont see how it isnt still predicting what token comes next. \n\n  \nIf it got the instruction \"Put the first word last and the last word first\" it just predicts what a last word for a sentence would be, then a second word, ... then first word. \n\n  \nEssentially, what comes next in the beginning would be something that looks like the end of a sentence. Then the thing that comes next looks like something that would be right after the first word of that same sentence, and by the time it gets to the last word, what comes next is whatever looks like the first word of the sentence. Its always just predecting the next thing.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1716176532,
    "created_utc": 1716176532,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l4tvid4",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1cvvbcq",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l4tvid4",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t3_1cvvbcq",
    "permalink": "/r/ChatGPT/comments/1cvvbcq/if_llms_are_just_next_token_prediction_how_are/l4tvid4/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1716176546,
    "saved": false,
    "score": 2,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 2,
    "user_reports": []
}