{
    "_meta": {
        "retrieved_2nd_on": 1716885651
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "Comprehensive-Tea711",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_87j2bwew",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": ">Parrots don't repeat well-crafted responses to human verbal input.\n\nThis is false. Parrots can be trained to reproduce sound patterns on que. So, a parrot could be trained to reproduce \"Socrates is mortal\" upon hearing \"All men are mortal. Socrates is man.\" (though I don't know of a specific test case for this, it is in theory not at all beyond the capability of a parrot, given that we do know they can reproduce interpretable sounds on que). \n\n>They just repeat stuff they have already heard. LLMs can repeat far more than the stuff they've heard before.\n\nTo some degree, of course, because LLMs are *stochastic* models! You can play with the temperature setting of the model through APIs of stuff like ChatGPT or Gemini. And when you stray too far from the training data, that's exactly why they fail miserably too.\n\n>\\[...\\] but that's not dissimilar from humans using their own knowledge when responding.\n\nRight, this is where people adopting your position always start to transform their claim into something that is utterly unfalsifiable. Any output from the LLM that is coherent is proof to them that the model can reason. Any output from the LLM that is incoherent, however, is not proof to them that the model cannot reason... because after all, humans also sometimes fail at reason. Just like you're failing right now, I suppose.... It's a very convenient and naive theory for proving to themselves that LLMs can reason.\n\nBut it would be a strawman to suggest that my argument is framed in a parallel way, as though I am simply saying that cases of a failure to reason is proof that they don't reason. My argument is that your entire approach here is very naive. And the reason for thinking that a set of mathematical algorithms doing matrix multiplication, soft-max, etc. is not reasoning is because (a) this has never been the definition of a ratiocinative process and (b) is not what we observe ourselves to be doing when we engage in a ratiocinative process and (c) cannot even account for ratiocinative processes like deduction. We do not have to posit the ratiocinative process in order to explain the outputs, because the mathematical explanation is sufficient.\n\n>Exactly, it does work in the same way as your calculator. In fact, there's nothing random about it, it's answer is always due to some objective logic.\n\nThis is the fallacy of equivocation. You're adopting a bizarre construal of the term in order to maintain your claim. No one talks about their calculator engaging in reason to produce its outputs and if that's all people mean, then the fact that an LLM reasoning would not be in the least bit controversial. In fact, since you think reasoning just means non-random output and you think everything is the product of non-random output... everything is reasoning.\n\nLook, if you want to try to protect your original claim by defining it into such a unusual construal, that's fine. I'm perfectly willing to acknowledge that you've stipulated a weird sense of reasoning that we can call reasoning\\* and then you've asserted that nothing is not random\\* (never mind the Copenhagen interpretation of quantum mechanics I guess). And if that is true (never mind how you know it's true), then sure, LLMs are reasoning\\*... because everything in the universe is reasoning\\* by your construal of the terms.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1716756039,
    "created_utc": 1716756039,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l5skpe3",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1d11m2o",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l5skpe3",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l5sghlp",
    "permalink": "/r/ChatGPT/comments/1d11m2o/difference_between_siri_and_chatgpt/l5skpe3/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1716756056,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}