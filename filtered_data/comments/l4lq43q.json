{
    "_meta": {
        "retrieved_2nd_on": 1716171438
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "Mysterious-Rent7233",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_yuor7gbbf",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "Of course he didn't say anything like that. He's a scientist, not a mechanic, operating at the far edges of the boundaries of human knowledge.\n\nThey don't know what they don't know and even Sam Altman would admit that.\n\nThey literally do not know how or why deep learning works. They do not know how or why LLMs work. They do not know what is going on inside of LLMs. Mathematical theory strongly suggests that LLMs and deep neural networks should not work. And yet they are doing something, but we don't know what, exactly.\n\nI can quote many industry experts saying those exact things, including OpenAI employees who are **not** on the safety team. Including Sam Altman.\n\nHis job is to make a thing that we do not understand, safe, while we are making it harder and harder to understand. It is as if Boeing is doubling the size of the jet every year and doesn't understand aerodynamics yet.\n\nThe [description](https://www.google.com/search?q=robert+miles+ai&oq=robert+miles+ai&gs_lcrp=EgZjaHJvbWUqBwgAEAAYgAQyBwgAEAAYgAQyBwgBEAAYgAQyCAgCEAAYFhgeMggIAxAAGBYYHjIICAQQABgWGB4yCAgFEAAYFhgeMgoIBhAAGIAEGKIEMgoIBxAAGIAEGKIEqAIAsAIA&sourceid=chrome&ie=UTF-8#:~:text=Search%20Results-,Robert%20Miles%20AI%20Safety,147.4K%2B%20followers,-Robert%20Miles%20AI) of [the](https://www.amazon.ca/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111) risk [is](https://www.youtube.com/watch?v=VihA_-8kBNg&t=3324s) out [in](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html) the [public](https://www.google.com/search?q=geoff+hinton+ai+kill+us&oq=geoff+hinton+ai+kill+us&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDUyNDVqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8#:~:text=%27Godfather%20of%20AI%27%20warns,May%202%2C%202023) domain. [We](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) don't [need](https://aiimpacts.org/stuart-russells-description-of-ai-risk/) a [whistleblower](https://www.businessinsider.com/elon-musk-20-percent-chance-ai-destroys-humanity-2024-3#:~:text=Elon%20Musk%20recalculated%20his%20cost%2Dbenefit%20analysis%20of%20AI's%20risk,the%20risk%20of%20potential%20catastrophe). They [wouldn't](https://www.cnn.com/2023/10/31/tech/sam-altman-ai-risk-taker/index.html#:~:text=Two%20weeks%20after%20the%20hearing,as%20pandemics%20and%20nuclear%20war.%E2%80%9D) tell [us](https://dig.watch/updates/ceo-of-anthropic-highlights-the-three-tiers-of-ai-risks) anything [we](https://www.google.com/search?q=ai+risk+max+tegmark&oq=ai+risk+max+tegmark&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRhA0gEJMTI4NzNqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8#:~:text=Max%20Tegmark%20explains,Nov%2027%2C%202023) don't already [know](https://time.com/6273743/thinking-that-could-doom-us-with-ai/).\n\nThe request is very simple, just like missing bolts: AI capability research should dramatically slow down. AI control and interpretability research should massively speed up and Sam Altman is doing the opposite of that.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1716041826,
    "created_utc": 1716041826,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l4lq43q",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1cuam3x",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l4lq43q",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l4kzp6d",
    "permalink": "/r/ChatGPT/comments/1cuam3x/openais_head_of_alignment_quit_saying_safety/l4lq43q/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1716041841,
    "saved": false,
    "score": 0,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 0,
    "user_reports": []
}