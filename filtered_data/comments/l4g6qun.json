{
    "_meta": {
        "is_edited": true,
        "retrieved_2nd_on": 1716080277
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "trajo123",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_3ofwm7j3",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "Interesting question, worth dissecting because there are some subtleties at play here. \n\n>Humans tend to understand discourse via context, social education, formal education, etc.\n\nHard to disagree with this one. This is the least debatable statement.\n\n>Thus we understand the meaning of a sentence.\n\nThis is starting to get shaky. Specifically the \"thus\" as it implies that understanding is purely related to experience and even more specifically to the modality of experience (social education, embodied experience, formal education, etc).   \nThe answer to this question hinges on the definition of \"understanding\" which is actually not trivial to define clearly. Ironically, just by starting from a basic definition given by GPT-4o, we can already see that there is more than experience when it comes to define understanding:\n\n>Understanding is a complex and multifaceted concept that refers to the cognitive process of comprehending, grasping, or making sense of information, ideas, experiences, or phenomena. It involves several key elements:\n\n>**Knowledge Acquisition**: Gathering relevant information through various means such as reading, observing, listening, and experiencing.\n\n>**Interpretation**: Analyzing and making sense of the information, often involving the ability to see patterns, make connections, and discern underlying principles or concepts.\n\n>**Integration**: Combining new information with existing knowledge, leading to a coherent and comprehensive grasp of the subject matter.\n\n>**Application**: Using the acquired and integrated knowledge to solve problems, make decisions, or perform tasks effectively.\n\n>**Explanation**: The ability to communicate or teach the knowledge to others, demonstrating a deep comprehension of the subject.\n\n>**Perspective-taking**: Recognizing different viewpoints and understanding how they influence interpretations and conclusions.\n\nThe whole conversation can be found [here](https://chatgpt.com/share/944b94f5-f7c4-404e-b9a3-fc6756efd61c).\n\n>Do LLMs truly understand the meaning and context behind the sentences they produce or is it only producing these tokens as a result of the next logical step?\n\nThis is the main question, but the way you phrase it is loaded. By saying \"only producing these tokens as a result of the next logical step\", you are already implying that that is not understanding, which is actually what the questions is about in the first place. While it may be obvious it's important to highlight that the question is not about whether AI models are alive or conscious or have emotions, it's about whether they understand language.\n\nSo let's look back at the definition above and see if an LLM meets these points:\n\n**Knowledge Acquisition**: yes, huge datasets of text and now audio and video. These AI models are not alive in the sense of biological homeostasis, and they don't acquire and process knowledge with the purpose of surviving in an environment or increase their chances of perpetuating their species, but they get fed a lot of knowledge. The modalities and substrate are different, but it's knowledge acquisition nonetheless.\n\n**Interpretation**: \"involving the ability to see patterns\". Deep learning and machine learning in general is all about finding patters in a data. LLMs in a sense compress information and that at a fundamental level requires the ability to find patterns.\n\n**Integration**: again, through the training process (back-propagation), all this huge corpus of information is integrated into a coherent model. This is mathematically the way training works: optimizing parameters such that a loss function decreases.\n\nNow we move from the \"input\" part of understanding to the \"output\" part. \n\n**Application**: For anyone using LLMs it is obvious that they can answer questions meaningfully. I can ask an LLM to write a program to calculate something and the code it provides very often just works, or is very close to a solution. It took what I asked, used it's knowledge to produce a result which can be objectively tested. Sometimes, when the provided code doesn't work, by giving it the error messages it can often identify what the mistake was.\n\n**Explanation**: \"The ability to communicate or teach the knowledge to others\" Again, this is empirically true. If anything, this is the most useful aspect of an LLM. To explain concepts with which the user is not familiar with. It is very useful to use as a learning companion. \n\n**Perspective-taking**: \"Recognizing different viewpoints and understanding how they influence interpretations and conclusions.\" Again, empirically true, and maybe this is the aspect of chatbots that most people find annoying, when they start moralizing and saying things like \"it is important to consider the nuances ....\". \n\nSo, my answer would be yes, LLMs definitely understand language, but not like humans do, because they do not experience the world in the same way and they are not alive. If that key difference is actually important is another questions. But for all intents and purposes, yes, LLMs understand language.\n\n\n\nTLDR: Yes, LLMs understand language, but not in the same way humans do.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1715950664,
    "created_utc": 1715950664,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l4g6qun",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1ctypcj",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l4g6qun",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t3_1ctypcj",
    "permalink": "/r/ChatGPT/comments/1ctypcj/do_llms_actually_understand_language_or_do_they/l4g6qun/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1715950679,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}