{
    "_meta": {
        "retrieved_2nd_on": 1714896644
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "sirtrogdor",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_9xvxw",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "This is just semantics. In a sense, calculators work in binary and don\u2019t calculate that 0+1=1, but rather have it memorized (physically written into their circuits).  \nI can't see why it\u2019s a such a huge difference that LLMs might have the sums of all of the single digit numbers memorized, along with the rules of addition. Don\u2019t you? Or do you count on your fingers?  \nIt\u2019s not even a guarantee that it\u2019s memorized even those tiny sums. A neural network regularly does actual addition and multiplication during evaluation and could easily encode numbers accordingly and so do \u201creal\u201d addition. Not that that would matter to me.  \nThe main actual difference is the accuracy. LLMs are prone to hallucinations for addition, as they are with all things, and so might only add 2+2 correctly with some 99% accuracy, because some 1% of the time someone said 2+2=5 as a joke, or something. Calculators do this with 100% accuracy. Humans do this with 99.99% accuracy. LLMs are also incapable of double checking their work or correcting their mistakes, so those small errors get compounded for larger problems. Poetry is more forgiving.\n\nIt\u2019s been proven that neural networks can learn to approximate arbitrary circuits, like the kind that make up a calculator.  \nI could hand-craft a neural net that did accurate addition for large digit numbers pretty easily. Even 100s or 1000s of digits.  \nI could do the same for multiplication, but since calculators get to use working memory for this, I would want to do the same for my LLM. But in this case, the only working memory the LLM has is the chat window. You would see it do its work in front of you.  \nMost LLMs don\u2019t naturally want to do their work in front of you, and try to do the full calculation with as few tokens as possible, because these LLMs are emulating human text, where we do all of our calculations off screen and seemingly pull answers out of thin air.\n\nAnyways, all that said, when I asked ChatGPT 3.5 (not this GPT) to give me the \u201clife\" number of 20 different random dates, it really didn\u2019t seem to have much trouble adding.  \nIt got 16 answers completely correct, 3 answers mostly correct (not due to an addition problem, it decided to reduce the master number 11 to 2, which you\u2019re apparently not supposed to do in numerology, I guess), and 1 answer wrong. That one answer it summed the digits of\u00a08/31/1985 as 8+3+1+9+8+5=34. Missed a 1. Again, not really a summation problem but a transcription problem. Likely due to tokenization. The same reason ChatGPT struggles to extract the 5th letter from a word.  \nHonestly I don\u2019t consider this accuracy that bad. There are plenty of non-math problems where ChatGPT is only right like 50% of the time. Or 10% of the time but maybe with usable outputs anyways.  \nI didn't check the accuracy of this GPT specifically, I suspect thanks to prompting it\u2019s probably higher.  \nEither way I don\u2019t think the accuracy is so bad as to call OP a dumb idiot. Folks were asking him to ignore his own senses and the several correct answers he got, without even trying it themselves, saying that LLMs were incapable of producing those kinds of answers. The ones he got...",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1714761231,
    "created_utc": 1714761231,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l2foizq",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1ciyypk",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l2foizq",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l2f5qje",
    "permalink": "/r/ChatGPT/comments/1ciyypk/need_a_numerology_reading_but_dont_want_to_pay/l2foizq/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1714761246,
    "saved": false,
    "score": 2,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 2,
    "user_reports": []
}