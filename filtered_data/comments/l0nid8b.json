{
    "_meta": {
        "is_edited": true,
        "removal_type": "removed",
        "retrieved_2nd_on": 1713864150,
        "was_deleted_later": true
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "SupportQuery",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_f6yhw7j2i",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "> It is an existential threat only if the human handling the AI is incredibly stupid\n\nLike you for example. Niave people like you are literally how it's going to bad. Case in point:\n\n> not creating an AI that can easily trick a human operator into privilege escalation and also human operators should be trained against it\n\nAgain, this is just incredibly naive. The Alignment Research Center recently posted a thought experiment in very specific, narrow example (eliciting latency knowledge from a system) with a 106 page paper enumerating every possible solution they'd come up with along with why it wouldn't work. They offered $50k prize for finding something they hadn't thought of. You should go claim it (\"just sandbox it!\", \"just supervise it\", \"just don't make one that can trick humans\", \"just train the operators\", etc.)  If you want to educate yourself first, so you understand the technical terms (this is a fairly mature field), you can watch that Computerphile series I mentioned. Or you can keep talking out of your butt. Whatever floats your boat.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1713734540,
    "created_utc": 1713734540,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l0nid8b",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1c9mpz9",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l0nid8b",
    "no_follow": false,
    "num_reports": null,
    "parent_id": "t1_l0nhaid",
    "permalink": "/r/ChatGPT/comments/1c9mpz9/warehouse_robot_collapses_after_working_for_20/l0nid8b/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1713734559,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}