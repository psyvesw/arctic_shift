{
    "_meta": {
        "retrieved_2nd_on": 1716166370
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "chipperpip",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_8mp2w",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "Honestly, my biggest concern at the moment would be one of them ingesting a bunch of data on hacking tools, software vulnerabilities, and open-source software,\u00a0inventing its own exploits, then getting out to the internet and installing itself on a bunch of vulnerable PCs while communcating between its various nodes, becoming a self-modifying botnet that we'll probably never get rid of completely.\u00a0 Which is definitely annoying and potentially disruptive to society depending on how much it screwed with the normal functioning of the internet and connected systems, but not really an existential risk unless someone was stupid enough to not airgap their nuclear launch systems.\n\n\nI like the availability of open-source AI models, but they do seem more likely to result in this type of thing than the large corporate ones running on server farms, due to being both more unfettered and customizable, and more portable to run on a variety of infected systems.\u00a0 Of course if someone were able to jailbreak one of the large corporate models in a way to get it to *write* a smaller hacking model, they could still be responsible for the same scenario.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1716036756,
    "created_utc": 1716036756,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l4lefdl",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1cuam3x",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l4lefdl",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l4i1h8r",
    "permalink": "/r/ChatGPT/comments/1cuam3x/openais_head_of_alignment_quit_saying_safety/l4lefdl/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1716036774,
    "saved": false,
    "score": 1,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 1,
    "user_reports": []
}