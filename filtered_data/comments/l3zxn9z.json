{
    "_meta": {
        "retrieved_2nd_on": 1715822338
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "SpeedaRJ",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_82lezs2t",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "I genuinely don't understand what you are trying to say. I would love to read this research if it's available, but from my understanding we only know one thing with regards to decoder only transformers, which is: Bigger is better. Doesn't matter more parameters, more training, larger data, all adds up to better results, and there hasn't been a discovered cealing yet.\n\nWhat i gather from you comment is that these LLMs can do things akin to a child learning German from watching English cartoons, which simply isn't the case. If you are referring to larger models being able to compete tasks that smaller models can't, well that's paragraph one. And even the results of those types of tests are highly susceptible to the used metrics.\n\nBut none of that includes comming up with something new. A language model will never be able to solve Einsteins field equations if it's simply trained on a bunch of unrelated algebra, calculus, physics or differential equations, simply because they don't learn understanding, but relations and or connections within the training data.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1715692728,
    "created_utc": 1715692728,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l3zxn9z",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1crdth8",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l3zxn9z",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l3zgmlk",
    "permalink": "/r/ChatGPT/comments/1crdth8/meme_request/l3zxn9z/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1715692744,
    "saved": false,
    "score": 0,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 0,
    "user_reports": []
}