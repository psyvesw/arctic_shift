{
    "_meta": {
        "is_edited": true,
        "retrieved_2nd_on": 1713874253
    },
    "all_awardings": [],
    "approved_at_utc": null,
    "approved_by": null,
    "archived": false,
    "associated_award": null,
    "author": "SupportQuery",
    "author_flair_background_color": null,
    "author_flair_css_class": null,
    "author_flair_richtext": [],
    "author_flair_template_id": null,
    "author_flair_text": null,
    "author_flair_text_color": null,
    "author_flair_type": "text",
    "author_fullname": "t2_f6yhw7j2i",
    "author_is_blocked": false,
    "author_patreon_flair": false,
    "author_premium": false,
    "awarders": [],
    "banned_at_utc": null,
    "banned_by": null,
    "body": "> That\u2019s not a brain.\n\n\\*facepalm\\* It's a metaphor.\n\n> It is a system composed of software and data\n\nIt doesn't matter what it's *composed* of. Unless you think brains are made of god magic, they're just physics.\n\n> is extremely simple compared to an actual brain\n\nThat is utter nonsense. No, they are not *et at the same *scale* of a human brain, but they are obscenely complex. You have no basis for making a relative comparison, because [we don't know how they work](https://www.google.com/search?q=we+don%27t+know+how+ai+work). The field of interoperability, if trying to figure out how they work, is in it's infancy and might go nowhere. It might literally be too complex for us to figure out. \n\n[This engineer created a *tiny* neural net (~400 parameters)](https://cprimozic.net/blog/reverse-engineering-a-small-neural-network/) trained to add binary numbers (very simple problem) then reverse engineering the solution. It turns out it did it by creating a digital to analog converter, which is insane. We find that learned solutions will often have this character. They're unlike anything we engineer, often taking advances of properties of the systems that we're completely unaware of.\n\nWe had the same issue what evolutionary algorithms. Daniel Hillis, who pioneered parallel computers in artificial intelligence, has a story of evolving a sorting algorithm, which is one of the simplest algorithms you can make:\n\n> One of the interesting things about the sorting programs that evolved in\nmy experiment is that I do not understand how they work. I have carefully\nexamined their instruction sequences, but I do not understand them: I have no\nsimpler explanation of how the programs work than the instruction sequences\nthemselves. It may be that the programs are not understandable\u2014that there is\nno way to break the operation of the program into a hierarchy of\nunderstandable parts. If this is true\u2014if evolution can produce something as\nsimple as a sorting program which is fundamentally incomprehensible\u2014it\ndoes not bode well for our prospects of ever understanding the human brain.\n\nThis situation is literally *billions* of times worse with LLM. Not only do we not know how they work, we might *never* know how they work. How does a bunch of weights between \"neurons\" learn to tell that a person has a mischievous look on their face? We don't know, any more than we understand how our own brains work. \n\nBoth evolutionary algorithms and neural nets are an example of *biomimicry*. We're literally copying nature in an attempt to reproduce things nature has produced, just like early flying machines were made by copying birds, long before we know anything about aeronautics.\n\nBy simply scaling up the number of artificial \"neurons\" these \"brains\" they're doing things that for the entire history of computing people said machines would never do. All thinking games (chess, go, ect) fell, then it could do art and music and language, all just by scaling the number of neurons up. Moreover, a lot of the capabilities we did not train it for. We trained it to predict English text, yet it learned every language. Both the degree of success and the nature of the capabilities that emerged were *unexpected*. The literature is full of research addressing this question. Why is it so effective? We literally really say, because we don't know how it works.\n\nSo to dismiss it as \"extremely simple\" is ignorant. And if you feel that's a quote mine, then I'd say dismissing it as \"extremely simple compared to an actual brain\" is equally ignorant. Rather, it's an unsupportable assertion, because we don't understand how *either* LLMs or brains work.\n\n> You might as well say that an airplane\u2019s autopilot system is sentient.\n\nYes, because saying LLMs do more than produce grammatical sentences is exactly equivalent to saying an autopilot is sentient.",
    "can_gild": false,
    "can_mod_post": false,
    "collapsed": false,
    "collapsed_because_crowd_control": null,
    "collapsed_reason": null,
    "collapsed_reason_code": null,
    "comment_type": null,
    "controversiality": 0,
    "created": 1713744640,
    "created_utc": 1713744640,
    "distinguished": null,
    "downs": 0,
    "edited": false,
    "gilded": 0,
    "gildings": {},
    "id": "l0o7j8k",
    "is_submitter": false,
    "likes": null,
    "link_id": "t3_1c9c62r",
    "locked": false,
    "mod_note": null,
    "mod_reason_by": null,
    "mod_reason_title": null,
    "mod_reports": [],
    "name": "t1_l0o7j8k",
    "no_follow": true,
    "num_reports": null,
    "parent_id": "t1_l0o1iqa",
    "permalink": "/r/ChatGPT/comments/1c9c62r/todays_ai_may_just_be_autocorrect_on_steroids_but/l0o7j8k/",
    "removal_reason": null,
    "replies": "",
    "report_reasons": null,
    "retrieved_on": 1713744655,
    "saved": false,
    "score": 2,
    "score_hidden": false,
    "send_replies": true,
    "stickied": false,
    "subreddit": "ChatGPT",
    "subreddit_id": "t5_7hqomg",
    "subreddit_name_prefixed": "r/ChatGPT",
    "subreddit_type": "public",
    "top_awarded_type": null,
    "total_awards_received": 0,
    "treatment_tags": [],
    "unrepliable_reason": null,
    "ups": 2,
    "user_reports": []
}